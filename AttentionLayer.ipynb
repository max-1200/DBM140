{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is coppied and adapted from the following webpage:\n",
    "# https://www.geeksforgeeks.org/emotion-detection-using-bidirectional-lstm/\n",
    "# The attention layer is coppied and adapted from the following page:\n",
    "# https://zhuanlan.zhihu.com/p/29201491?utm_id=0\n",
    "# https://github.com/stevewyl/nlp_toolkit/tree/master/nlp_toolkit/modules/attentions\n",
    "# https://github.com/sujitpal/eeap-examples/blob/master/src/custom_attn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\4K\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,Bidirectional\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from keras.layers import *\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('text_emotion_isear_edit2.csv', header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMOTION\n",
      "fear       1074\n",
      "sadness    1068\n",
      "anger      1062\n",
      "joy        1061\n",
      "shame      1060\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Changed a bit as now we have a DataFrame\n",
    "df.drop(df[df['SIT'] == '[ No response.]'].index, inplace = True)\n",
    "#let's get rid of anger and disgust to have a try\n",
    "df = df[~df['EMOTION'].isin(['guilt', 'disgust'])]\n",
    "print (df['EMOTION'].value_counts() )\n",
    "\n",
    "number_emotions = df['EMOTION'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'an', 'event', 'i', 'will', 'never', 'forget', '.', 'i', 'am', 'considered', 'a', 'good', 'ã¡', 'mimic', '.', 'this', 'particular', 'day', 'we', 'were', 'waiting', 'for', 'a', 'professor', 'ã¡', 'to', 'take', 'her', 'class', '.', 'this', 'professor', 'had', 'a', 'peculiar', 'accent', 'and', 'a', 'ã¡', 'very', 'horrible', 'way', 'of', 'lecturing', 'and', 'to', 'top', 'it', 'all', 'we', 'had', 'this', 'ã¡', 'class', 'at', 'the', 'fag', 'end', 'of', 'the', 'day', '.', 'so', 'this', 'particular', 'day', 'i', 'got', 'ã¡', 'into', 'my', 'element', 'and', 'started', 'imitating', 'the', 'professor', ',', 'and', 'the', 'ã¡', 'professor', 'entered', 'the', 'class', 'right', 'in', 'the', 'middle', 'of', 'it', '.', 'i', 'was', 'ã¡', 'unaware', 'of', 'her', 'presence', ',', 'some', 'of', 'my', 'friends', 'tried', 'to', 'warn', 'me', 'but', 'ã¡', 'it', 'was', 'of', 'no', 'avail', 'as', 'i', 'was', 'too', 'engrossed', 'in', 'mimicing', '.', 'then', 'i', 'ã¡', 'suddenly', 'noticed', 'the', 'silence', 'and', 'turned', 'around', 'to', 'see', 'her', 'ã¡', 'entering', 'the', 'class', '.', 'i', 'do', 'not', 'know', 'till', 'today', 'if', 'she', 'actually', 'saw', 'ã¡', 'what', 'i', 'had', 'been', 'doing', 'or', 'she', 'had', 'completely', 'ignored', 'it', '.', 'through', 'ã¡', 'out', 'that', 'lecture', 'i', 'died', 'of', 'shame', '.', 'all', 'the', 'more', 'shameful', 'was', 'the', 'ã¡', 'fact', 'that', 'she', 'gave', 'me', 'a', 'character', 'certificate', '.']\n",
      "Sublist with the most elements: 192\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#make all contexts (sentences) in dataset a list\n",
    "texts =df[\"SIT\"]\n",
    "\n",
    "#HIGHTLIGHT\n",
    "#Convert each text value to lowercase using the .str.lower() method\n",
    "lower_texts = texts.str.lower()\n",
    "\n",
    "# Each  sentence in feel_arr is tokenized by the help of work tokenizer.\n",
    "# If I have a sentence - 'I am happy'. \n",
    "# After word tokenizing it will convert into- ['I','am','happy']\n",
    "lower_texts = [word_tokenize(sent) for sent in lower_texts]\n",
    "\n",
    "#Now I added some codes to see how many words it has for the longest text\n",
    "longest_text = max(lower_texts, key=len)\n",
    "print (longest_text)\n",
    "print(\"Sublist with the most elements:\", len(longest_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defined a function padd in which each sentence length is fixed to 200.\n",
    "# If length is less than 200 , then the word- '<padd>' is append\n",
    "def padd(arr):\n",
    "    for i in range(200-len(arr)):\n",
    "        arr.append('<pad>')\n",
    "    return arr[:200]\n",
    "   \n",
    "# call the padd function for each sentence in feel_arr\n",
    "for i in range(len(lower_texts)):\n",
    "    lower_texts[i]=padd(lower_texts[i])\n",
    " \n",
    "\n",
    "# Glove vector contains a 50 dimensional vector corresponding \n",
    "# to each word in dictionary.\n",
    "vocab_f = 'glove.6B.50d.txt'\n",
    " \n",
    "# embeddings_index is a dictionary which contains the mapping of\n",
    "# word with its corresponding 50d vector.\n",
    "embeddings_index = {}\n",
    "with open(vocab_f, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        # splitting each line of the glove.6B.50d in a list of \n",
    "        # items- in which the first element is the word to be embedded,\n",
    "        # and from second to the end of line contains the 50d vector.\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "         \n",
    "# Embedding each word of the feel_arr\n",
    "embedding_texts = []\n",
    "# Iterate through each sub-list (sentences)\n",
    "for word_list in lower_texts:\n",
    "    # Initialize an empty list to store the embedding vectors for each word\n",
    "    word_vectors = []\n",
    "    for word in word_list:\n",
    "        if word in embeddings_index:\n",
    "            word_vector = embeddings_index[word]\n",
    "            word_vectors.append(word_vector)\n",
    "        else:\n",
    "            # if the word to be embedded is '<padd>' append 0 fifty times\n",
    "            word_vector = np.zeros(50, dtype='float32')\n",
    "            word_vectors.append(word_vector)\n",
    "    embedding_texts.append(word_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeeding_texts cannot be a list, for the website he made it NumPy-array\n",
    "X = np.array(embedding_texts)\n",
    "\n",
    "#One-Hot Encoding for all emotions, \n",
    "#instead of just give them numbers, by doing so we will have seven outputs\n",
    "Y = pd.get_dummies(df['EMOTION'], prefix='emotion')\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass BiLSTMModel:\\n    def __init__(self):\\n        self.model = Sequential()\\n        self.model.add(Input(shape=(200, 50)))\\n        \\n        self.model.add(Bidirectional(LSTM(64, input_shape=(200, 50))))\\n\\n        self.model.add(Dropout(0.2))\\n        \\n        self.model.add(Dense(5, activation='softmax'))\\n        self.model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\\n \\n    def fit(self, X, Y, epochs, batch_size,verbose):\\n        self.model.fit(X, Y, epochs=epochs, batch_size=batch_size,verbose=verbose)\\n \\n    def evaluate(self, X, Y, batch_size,verbose):\\n        return self.model.evaluate(X, Y, batch_size=batch_size,verbose=verbose)\\n \\n    def predict(self, X):\\n        return self.model.predict(X)\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class BiLSTMModel:\n",
    "    def __init__(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Input(shape=(200, 50)))\n",
    "        \n",
    "        self.model.add(Bidirectional(LSTM(64, input_shape=(200, 50))))\n",
    "\n",
    "        self.model.add(Dropout(0.2))\n",
    "        \n",
    "        self.model.add(Dense(5, activation='softmax'))\n",
    "        self.model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    " \n",
    "    def fit(self, X, Y, epochs, batch_size,verbose):\n",
    "        self.model.fit(X, Y, epochs=epochs, batch_size=batch_size,verbose=verbose)\n",
    " \n",
    "    def evaluate(self, X, Y, batch_size,verbose):\n",
    "        return self.model.evaluate(X, Y, batch_size=batch_size,verbose=verbose)\n",
    " \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class Attention(Layer):\n",
    "    def __init__(self, attention_size, **kwargs):\n",
    "        self.attention_size = attention_size\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # W: (EMBED_SIZE, ATTENTION_SIZE)\n",
    "        # b: (ATTENTION_SIZE, 1)\n",
    "        # u: (ATTENTION_SIZE, 1)\n",
    "        self.W = self.add_weight(name=\"W_{:s}\".format(self.name),\n",
    "                                 shape=(input_shape[-1], self.attention_size),\n",
    "                                 initializer=\"glorot_normal\",\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name=\"b_{:s}\".format(self.name),\n",
    "                                 shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\",\n",
    "                                 trainable=True)\n",
    "        self.u = self.add_weight(name=\"u_{:s}\".format(self.name),\n",
    "                                 shape=(self.attention_size, 1),\n",
    "                                 initializer=\"glorot_normal\",\n",
    "                                 trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # input: (BATCH_SIZE, MAX_TIMESTEPS, EMBED_SIZE)\n",
    "        # et: (BATCH_SIZE, MAX_TIMESTEPS, ATTENTION_SIZE)\n",
    "        et = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        # at: (BATCH_SIZE, MAX_TIMESTEPS)\n",
    "        at = K.softmax(K.squeeze(K.dot(et, self.u), axis=-1))\n",
    "        if mask is not None:\n",
    "            at *= K.cast(mask, K.floatx())\n",
    "        # ot: (BATCH_SIZE, MAX_TIMESTEPS, EMBED_SIZE)\n",
    "        atx = K.expand_dims(at, axis=-1)\n",
    "        ot = atx * x\n",
    "        # output: (BATCH_SIZE, EMBED_SIZE)\n",
    "        output = K.sum(ot, axis=1)\n",
    "        return output\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, Bidirectional, LSTM, Input, Dense\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "\n",
    "input_layer = Input(shape=(200, 50))\n",
    "\n",
    "# 创建一个Bidirectional LSTM层\n",
    "lstm_layer = Bidirectional(LSTM(64, return_sequences=True))(input_layer)\n",
    "\n",
    "# 创建一个注意力层\n",
    "attention = Attention(5)(lstm_layer)\n",
    "\n",
    "# 在注意力层之后添加其他层，例如全连接层\n",
    "output_layer = tf.keras.layers.Dense(5, activation=\"softmax\")(attention)\n",
    "\n",
    "# 创建模型\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 200, 50)]         0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 200, 128)          58880     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " attention_1 (Attention)     (None, 128)               845       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60370 (235.82 KB)\n",
      "Trainable params: 60370 (235.82 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model = BiLSTMModel()\n",
    "# Take a look of the model layers summarry\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMModel:\n",
    "    def __init__(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Input(shape=(200, 50)))\n",
    "        \n",
    "        self.model.add(Bidirectional(LSTM(64, input_shape=(200, 50))))\n",
    "\n",
    "        self.model.add(Dropout(0.2))\n",
    "        \n",
    "        self.model.add(Dense(5, activation='softmax'))\n",
    "        self.model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    " \n",
    "    def fit(self, X, Y, epochs, batch_size,verbose):\n",
    "        self.model.fit(X, Y, epochs=epochs, batch_size=batch_size,verbose=verbose)\n",
    " \n",
    "    def evaluate(self, X, Y, batch_size,verbose):\n",
    "        return self.model.evaluate(X, Y, batch_size=batch_size,verbose=verbose)\n",
    " \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13\n",
      "267/267 [==============================] - 11s 35ms/step - loss: 1.4898 - accuracy: 0.3404\n",
      "Epoch 2/13\n",
      "267/267 [==============================] - 9s 34ms/step - loss: 1.2137 - accuracy: 0.5094\n",
      "Epoch 3/13\n",
      "267/267 [==============================] - 9s 34ms/step - loss: 1.0881 - accuracy: 0.5732\n",
      "Epoch 4/13\n",
      "267/267 [==============================] - 10s 38ms/step - loss: 0.9773 - accuracy: 0.6246\n",
      "Epoch 5/13\n",
      "267/267 [==============================] - 9s 35ms/step - loss: 0.8926 - accuracy: 0.6582\n",
      "Epoch 6/13\n",
      "267/267 [==============================] - 9s 35ms/step - loss: 0.8208 - accuracy: 0.6901\n",
      "Epoch 7/13\n",
      "267/267 [==============================] - 9s 35ms/step - loss: 0.7680 - accuracy: 0.7099\n",
      "Epoch 8/13\n",
      "267/267 [==============================] - 9s 35ms/step - loss: 0.7116 - accuracy: 0.7331\n",
      "Epoch 9/13\n",
      "267/267 [==============================] - 9s 34ms/step - loss: 0.6676 - accuracy: 0.7474\n",
      "Epoch 10/13\n",
      "267/267 [==============================] - 9s 34ms/step - loss: 0.6141 - accuracy: 0.7662\n",
      "Epoch 11/13\n",
      "267/267 [==============================] - 9s 35ms/step - loss: 0.5538 - accuracy: 0.7944\n",
      "Epoch 12/13\n",
      "267/267 [==============================] - 9s 34ms/step - loss: 0.5189 - accuracy: 0.8138\n",
      "Epoch 13/13\n",
      "267/267 [==============================] - 9s 34ms/step - loss: 0.4659 - accuracy: 0.8371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1dc7166c7c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model on the input and target data\n",
    "model.fit(X_train,Y_train, epochs=13, batch_size=16,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0856750011444092\n",
      "Accuracy: 0.6422535181045532\n"
     ]
    }
   ],
   "source": [
    "#now we can check the performance of the model with the testing sets\n",
    "loss,accuracy = model.evaluate(X_test, Y_test, 16,verbose = 0)\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "Train Accuracy: 0.3390, Test Accuracy: 0.3315\n",
      "Epoch 2/15\n",
      "Train Accuracy: 0.4770, Test Accuracy: 0.4939\n",
      "Epoch 3/15\n",
      "Train Accuracy: 0.5308, Test Accuracy: 0.5502\n",
      "Epoch 4/15\n",
      "Train Accuracy: 0.5897, Test Accuracy: 0.5850\n",
      "Epoch 5/15\n",
      "Train Accuracy: 0.6174, Test Accuracy: 0.5991\n",
      "Epoch 6/15\n",
      "Train Accuracy: 0.6516, Test Accuracy: 0.6254\n",
      "Epoch 7/15\n",
      "Train Accuracy: 0.6721, Test Accuracy: 0.6376\n",
      "Epoch 8/15\n",
      "Train Accuracy: 0.6862, Test Accuracy: 0.6254\n",
      "Epoch 9/15\n",
      "Train Accuracy: 0.7094, Test Accuracy: 0.6347\n",
      "Epoch 10/15\n",
      "Train Accuracy: 0.7209, Test Accuracy: 0.6235\n",
      "Epoch 11/15\n",
      "Train Accuracy: 0.7418, Test Accuracy: 0.6357\n",
      "Epoch 12/15\n",
      "Train Accuracy: 0.7549, Test Accuracy: 0.6197\n",
      "Epoch 13/15\n",
      "Train Accuracy: 0.7669, Test Accuracy: 0.6413\n",
      "Epoch 14/15\n",
      "Train Accuracy: 0.7944, Test Accuracy: 0.6263\n",
      "Epoch 15/15\n",
      "Train Accuracy: 0.8038, Test Accuracy: 0.6263\n"
     ]
    }
   ],
   "source": [
    "# Define the number of training epochs and batch size\n",
    "epochs = 15\n",
    "batch_size = 16\n",
    "\n",
    "# Create empty lists to store training and test accuracy\n",
    "train_accuracy_history = []\n",
    "test_accuracy_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train the model for one epoch\n",
    "    history = model.fit(X_train,Y_train, epochs=1, batch_size=16,verbose=0)\n",
    "    # Calculate training accuracy\n",
    "    train_accuracy = history.history['accuracy'][0]\n",
    "    train_accuracy_history.append(train_accuracy)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=0)\n",
    "    test_accuracy_history.append(test_accuracy)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    print(f'Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion_anger</th>\n",
       "      <th>emotion_fear</th>\n",
       "      <th>emotion_joy</th>\n",
       "      <th>emotion_sadness</th>\n",
       "      <th>emotion_shame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7106</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6022</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2962</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3721</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4032</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1065 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      emotion_anger  emotion_fear  emotion_joy  emotion_sadness  emotion_shame\n",
       "1144           True         False        False            False          False\n",
       "7106          False         False        False            False           True\n",
       "296            True         False        False            False          False\n",
       "3             False         False        False             True          False\n",
       "1313          False         False        False            False           True\n",
       "...             ...           ...          ...              ...            ...\n",
       "6022          False         False        False            False           True\n",
       "2962          False         False        False            False           True\n",
       "3721          False          True        False            False          False\n",
       "1035          False          True        False            False          False\n",
       "4032          False         False         True            False          False\n",
       "\n",
       "[1065 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 1s 16ms/step\n",
      "Accuracy (Emotion 0): 0.56\n",
      "Accuracy (Emotion 1): 0.70\n",
      "Accuracy (Emotion 2): 0.67\n",
      "Accuracy (Emotion 3): 0.62\n",
      "Accuracy (Emotion 4): 0.65\n"
     ]
    }
   ],
   "source": [
    "# Prediction using models\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Initialize a dictionary to store the number of correct predictions and the total number of correct predictions for each sentiment classification\n",
    "accuracy_dict = {0: [0, 0], 1: [0, 0], 2: [0, 0], 3: [0, 0], 4: [0, 0]}\n",
    "\n",
    "\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "true_labels = []\n",
    "for index, row in Y_test.iterrows():\n",
    "    true_label_index = np.argmax(row.to_numpy())  # Index for finding the sentiment classification with the highest probability\n",
    "    true_labels.append(true_label_index)\n",
    "\n",
    "true_labels = np.array(true_labels)\n",
    "\n",
    "# Iterate over test data and labels to calculate accuracy\n",
    "for i in range(len(X_test)):\n",
    "    true_label = true_labels [i]\n",
    "    predicted_label = predicted_labels[i]\n",
    "    accuracy_dict[true_label][1] += 1  \n",
    "    if true_label == predicted_label:\n",
    "        accuracy_dict[true_label][0] += 1  \n",
    "\n",
    "# Calculate accuracy for each sentiment classification and print the results\n",
    "for emotion, [correct, total] in accuracy_dict.items():\n",
    "    if total > 0:\n",
    "        accuracy = correct / total\n",
    "        print(f\"Accuracy (Emotion {emotion}): {accuracy:.2f}\")\n",
    "    else:\n",
    "        print(f\"No test data for Emotion {emotion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it a DataFrame\n",
    "merged_train_accuracy_history = df = pd.DataFrame({\n",
    "    'train_accuracy_history': train_accuracy_history,\n",
    "    'test_accuracy_history': test_accuracy_history\n",
    "})\n",
    "\n",
    "\n",
    "# Export data to CSV file\n",
    "file_name = f\"S_accuracy_epochs_{epochs}_batch_{batch_size}.csv\"\n",
    "df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\python3.9\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "#Uncomment this piece of code if you want to save the model \n",
    "\n",
    "model.save('1trained_model_Sikai_20231020_1.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "#del model  # deletes the existing model\n",
    "    \n",
    "#returns a compiled model\n",
    "#identical to the previous one\n",
    "#model = load_model('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
