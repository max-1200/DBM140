{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is coppied and adapted from the following webpage:\n",
    "# https://www.geeksforgeeks.org/emotion-detection-using-bidirectional-lstm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\20193043\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,Bidirectional\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from keras.layers import *\n",
    "from sklearn.model_selection import cross_val_score \n",
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous code for import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from py_isear.isear_loader import IsearLoader\n",
    "attributes = ['EMOT','SIT']\n",
    "target = ['TROPHO','TEMPER']\n",
    "loader = IsearLoader(attributes, target, True)\n",
    "data = loader.load_isear('isear.csv')\n",
    "\n",
    "data.drop(data[data[1] == '[ No response.]'].index, inplace = True)\n",
    "print(data.head())\n",
    "#Hide it for now or the github page always takes forever to browse ;)\n",
    "#data.get_data() # returns attributes\n",
    "#data.get_target() # returns target\n",
    "#data.get_freetext_content() # returns the text content of the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMOTION</th>\n",
       "      <th>SIT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joy</td>\n",
       "      <td>During the period of falling in love, each tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fear</td>\n",
       "      <td>When I was involved in a traffic accident.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger</td>\n",
       "      <td>When I was driving home after  several days of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sadness</td>\n",
       "      <td>When I lost the person who meant the most to me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disgust</td>\n",
       "      <td>The time I knocked a deer down - the sight of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7421</th>\n",
       "      <td>anger</td>\n",
       "      <td>Two years back someone invited me to be the tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7422</th>\n",
       "      <td>sadness</td>\n",
       "      <td>I had taken the responsibility to do something...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7423</th>\n",
       "      <td>disgust</td>\n",
       "      <td>I was at home and I heard a loud sound of spit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7424</th>\n",
       "      <td>shame</td>\n",
       "      <td>I did not do the homework that the teacher had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7425</th>\n",
       "      <td>guilt</td>\n",
       "      <td>I had shouted at my younger brother and he was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7426 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      EMOTION                                                SIT\n",
       "0         joy  During the period of falling in love, each tim...\n",
       "1        fear         When I was involved in a traffic accident.\n",
       "2       anger  When I was driving home after  several days of...\n",
       "3     sadness  When I lost the person who meant the most to me. \n",
       "4     disgust  The time I knocked a deer down - the sight of ...\n",
       "...       ...                                                ...\n",
       "7421    anger  Two years back someone invited me to be the tu...\n",
       "7422  sadness  I had taken the responsibility to do something...\n",
       "7423  disgust  I was at home and I heard a loud sound of spit...\n",
       "7424    shame  I did not do the homework that the teacher had...\n",
       "7425    guilt  I had shouted at my younger brother and he was...\n",
       "\n",
       "[7426 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('text_emotion_isear_edit2.csv', header=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMOTION</th>\n",
       "      <th>SIT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joy</td>\n",
       "      <td>During the period of falling in love, each tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fear</td>\n",
       "      <td>When I was involved in a traffic accident.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger</td>\n",
       "      <td>When I was driving home after  several days of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sadness</td>\n",
       "      <td>When I lost the person who meant the most to me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disgust</td>\n",
       "      <td>The time I knocked a deer down - the sight of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7421</th>\n",
       "      <td>anger</td>\n",
       "      <td>Two years back someone invited me to be the tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7422</th>\n",
       "      <td>sadness</td>\n",
       "      <td>I had taken the responsibility to do something...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7423</th>\n",
       "      <td>disgust</td>\n",
       "      <td>I was at home and I heard a loud sound of spit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7424</th>\n",
       "      <td>shame</td>\n",
       "      <td>I did not do the homework that the teacher had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7425</th>\n",
       "      <td>guilt</td>\n",
       "      <td>I had shouted at my younger brother and he was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7426 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      EMOTION                                                SIT\n",
       "0         joy  During the period of falling in love, each tim...\n",
       "1        fear         When I was involved in a traffic accident.\n",
       "2       anger  When I was driving home after  several days of...\n",
       "3     sadness  When I lost the person who meant the most to me. \n",
       "4     disgust  The time I knocked a deer down - the sight of ...\n",
       "...       ...                                                ...\n",
       "7421    anger  Two years back someone invited me to be the tu...\n",
       "7422  sadness  I had taken the responsibility to do something...\n",
       "7423  disgust  I was at home and I heard a loud sound of spit...\n",
       "7424    shame  I did not do the homework that the teacher had...\n",
       "7425    guilt  I had shouted at my younger brother and he was...\n",
       "\n",
       "[7426 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Changed a bit as now we have a DataFrame\n",
    "df.drop(df[df['SIT'] == '[ No response.]'].index, inplace = True)\n",
    "df\n",
    "#this does not work since the structure of the ISEAR file on the website is very different than the one we use, the one we use has a lot of additional data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HIGHTLIGHT#HIGHTLIGHT#HIGHTLIGHT#HIGHTLIGHT#HIGHTLIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       During the period of falling in love, each tim...\n",
      "1              When I was involved in a traffic accident.\n",
      "2       When I was driving home after  several days of...\n",
      "3       When I lost the person who meant the most to me. \n",
      "4       The time I knocked a deer down - the sight of ...\n",
      "                              ...                        \n",
      "7421    Two years back someone invited me to be the tu...\n",
      "7422    I had taken the responsibility to do something...\n",
      "7423    I was at home and I heard a loud sound of spit...\n",
      "7424    I did not do the homework that the teacher had...\n",
      "7425    I had shouted at my younger brother and he was...\n",
      "Name: SIT, Length: 7426, dtype: object\n",
      "0       during the period of falling in love, each tim...\n",
      "1              when i was involved in a traffic accident.\n",
      "2       when i was driving home after  several days of...\n",
      "3       when i lost the person who meant the most to me. \n",
      "4       the time i knocked a deer down - the sight of ...\n",
      "                              ...                        \n",
      "7421    two years back someone invited me to be the tu...\n",
      "7422    i had taken the responsibility to do something...\n",
      "7423    i was at home and i heard a loud sound of spit...\n",
      "7424    i did not do the homework that the teacher had...\n",
      "7425    i had shouted at my younger brother and he was...\n",
      "Name: SIT, Length: 7426, dtype: object\n",
      "['a', 'few', 'days', 'back', 'i', 'was', 'waiting', 'for', 'the', 'bus', 'at', 'the', 'bus', 'stop', '.', 'ã¡', 'before', 'getting', 'into', 'the', 'bus', 'i', 'had', 'prepared', 'the', 'exact', 'amount', 'of', 'ã¡', 'coins', 'to', 'pay', 'for', 'the', 'bus', 'fair', 'and', 'when', 'i', 'got', 'into', 'the', 'bus', 'i', 'put', 'ã¡', 'these', 'coins', 'into', 'the', 'box', 'meant', 'to', 'collect', 'the', 'bus', 'fair', '.', 'i', 'ã¡', 'thought', 'that', 'i', 'had', 'paid', 'and', 'wanted', 'to', 'get', 'inside', '.', 'however', 'the', 'ã¡', 'bus', 'driver', 'called', 'me', 'and', 'asked', 'me', 'in', 'an', 'impolite', 'way', 'if', 'the', 'coins', 'ã¡', 'were', 'stuck', 'at', 'the', 'opening', 'of', 'the', 'box', '.', 'he', 'had', 'not', 'seen', 'me', 'paying', 'ã¡', 'and', 'there', 'was', \"n't\", 'a', 'stack', 'of', 'coins', 'in', 'the', 'box', '.', 'i', 'could', 'not', 'ã¡', 'understand', 'this', 'and', 'the', 'driver', 'kept', 'questioning', 'me', '.', 'he', 'made', 'me', 'ã¡', 'feel', 'angry', 'and', 'at', 'last', 'i', 'inserted', 'a', 'dollar', 'coin', 'in', 'the', 'box', 'just', 'ã¡', 'to', 'get', 'away', 'from', 'him', '.', 'later', 'i', 'found', 'that', 'i', 'had', 'forgotten', 'a', 'few', 'ã¡', 'coins', 'in', 'my', 'pocket', 'and', 'had', 'not', 'paid', 'enough', 'for', 'the', 'fair', 'the', 'first', 'ã¡', 'time', '.', 'after', 'i', 'had', 'entered', 'the', 'bus', 'i', 'could', 'still', 'hear', 'him', 'ã¡', 'scolding', 'me', 'and', 'i', 'felt', 'disgusted', '.']\n",
      "Sublist with the most elements: 201\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#make all contexts (sentences) in dataset a list\n",
    "texts =df[\"SIT\"]\n",
    "print(texts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#HIGHTLIGHT\n",
    "#Convert each text value to lowercase using the .str.lower() method\n",
    "lower_texts = texts.str.lower()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(lower_texts)\n",
    "# Each  sentence in feel_arr is tokenized by the help of work tokenizer.\n",
    "# If I have a sentence - 'I am happy'. \n",
    "# After word tokenizing it will convert into- ['I','am','happy']\n",
    "lower_texts = [word_tokenize(sent) for sent in lower_texts]\n",
    "\n",
    "\n",
    "#Now I added some codes to see how many words it has for the longest text\n",
    "longest_text = max(lower_texts, key=len)\n",
    "print (longest_text)\n",
    "print(\"Sublist with the most elements:\", len(longest_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow we don't have the same dataset as our longest text has 200 words. <br>\n",
    "for the next part I two choices: <br>\n",
    "1. change the length for all sentences into 200, then it takes more time to train the model <br>\n",
    "2. drop all texts more than certain words, 100 for instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to try the first way to see if its doable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['during', 'the', 'period', 'of', 'falling', 'in', 'love', ',', 'each', 'time', 'that', 'we', 'met', 'and', 'ã¡', 'especially', 'when', 'we', 'had', 'not', 'met', 'for', 'a', 'long', 'time', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Defined a function padd in which each sentence length is fixed to 120.\n",
    "# If length is less than 200 , then the word- '<padd>' is append\n",
    "def padd(arr):\n",
    "    for i in range(200-len(arr)):\n",
    "        arr.append('<pad>')\n",
    "    return arr[:200]\n",
    "   \n",
    "# call the padd function for each sentence in feel_arr\n",
    "for i in range(len(lower_texts)):\n",
    "    lower_texts[i]=padd(lower_texts[i])\n",
    " \n",
    "print(lower_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.013441,  0.23682 , -0.16899 ,  0.40951 ,  0.63812 ,  0.47709 ,\n",
       "       -0.42852 , -0.55641 , -0.364   , -0.23938 ,  0.13001 , -0.063734,\n",
       "       -0.39575 , -0.48162 ,  0.23291 ,  0.090201, -0.13324 ,  0.078639,\n",
       "       -0.41634 , -0.15428 ,  0.10068 ,  0.48891 ,  0.31226 , -0.1252  ,\n",
       "       -0.037512, -1.5179  ,  0.12612 , -0.02442 , -0.042961, -0.28351 ,\n",
       "        3.5416  , -0.11956 , -0.014533, -0.1499  ,  0.21864 , -0.33412 ,\n",
       "       -0.13872 ,  0.31806 ,  0.70358 ,  0.44858 , -0.080262,  0.63003 ,\n",
       "        0.32111 , -0.46765 ,  0.22786 ,  0.36034 , -0.37818 , -0.56657 ,\n",
       "        0.044691,  0.30392 ], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Glove vector contains a 50 dimensional vector corresponding \n",
    "# to each word in dictionary.\n",
    "vocab_f = 'glove.6B.50d.txt'\n",
    " \n",
    "# embeddings_index is a dictionary which contains the mapping of\n",
    "# word with its corresponding 50d vector.\n",
    "embeddings_index = {}\n",
    "with open(vocab_f, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        # splitting each line of the glove.6B.50d in a list of \n",
    "        # items- in which the first element is the word to be embedded,\n",
    "        # and from second to the end of line contains the 50d vector.\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "         \n",
    "# the embedding index of ','\n",
    "embeddings_index[',']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.29784   -0.018422  -0.71891   -0.4651    -0.45661   -0.0042153\n",
      " -0.74598    0.34662   -0.51781   -0.5877     0.18398   -0.36903\n",
      " -0.52225   -0.14082    0.83446   -0.26962   -0.89364   -0.11813\n",
      " -1.3076     0.475      0.52815   -0.021974   0.61869   -0.65362\n",
      " -0.14298   -1.6466    -0.05305   -0.17046    0.17048    0.75756\n",
      "  3.5832     0.13775   -0.37811   -0.48736    0.0069906  0.59913\n",
      "  0.31404    0.30734   -0.42397    0.35383   -0.97151    0.16082\n",
      " -0.63666   -0.20449   -0.070846  -0.32219   -0.049254  -0.41865\n",
      " -0.6899    -0.54908  ]\n"
     ]
    }
   ],
   "source": [
    "# Embedding each word of the feel_arr\n",
    "embedding_texts = []\n",
    "# Iterate through each sub-list (sentences)\n",
    "for word_list in lower_texts:\n",
    "    # Initialize an empty list to store the embedding vectors for each word\n",
    "    word_vectors = []\n",
    "    for word in word_list:\n",
    "        if word in embeddings_index:\n",
    "            word_vector = embeddings_index[word]\n",
    "            word_vectors.append(word_vector)\n",
    "        else:\n",
    "            # if the word to be embedded is '<padd>' append 0 fifty times\n",
    "            word_vector = np.zeros(50, dtype='float32')\n",
    "            word_vectors.append(word_vector)\n",
    "    embedding_texts.append(word_vectors)\n",
    "\n",
    "print(embedding_texts[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!!PROBLEM!!!!!<br>\n",
    "I checked and found the the GloVe cannot recognize the capital letters. <br>\n",
    "I can transfer all the capital letters into lowercase but the problem is for text message I personally uer capital letters a lot for emotion presentation<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data should be ready (FINALLY) for training <br>\n",
    "but still we need to coupling those random numbers with emotion (Input) <br>\n",
    "Also we need to divide the data into training and testing sets. <br>\n",
    "There are plenty of ways to divide, but to make it simple, I'll just make it 8:2 randomized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Input, we can add more if we have more time.<br>\n",
    "For instance, the length of the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print (type(embedding_texts))\n",
    "\n",
    "#embeeding_texts cannot be a list, for the website he made it NumPy-array\n",
    "X = np.array(embedding_texts)\n",
    "print (type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion_anger</th>\n",
       "      <th>emotion_disgust</th>\n",
       "      <th>emotion_fear</th>\n",
       "      <th>emotion_guilt</th>\n",
       "      <th>emotion_joy</th>\n",
       "      <th>emotion_sadness</th>\n",
       "      <th>emotion_shame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7421</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7422</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7423</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7424</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7425</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7426 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      emotion_anger  emotion_disgust  emotion_fear  emotion_guilt  \\\n",
       "0             False            False         False          False   \n",
       "1             False            False          True          False   \n",
       "2              True            False         False          False   \n",
       "3             False            False         False          False   \n",
       "4             False             True         False          False   \n",
       "...             ...              ...           ...            ...   \n",
       "7421           True            False         False          False   \n",
       "7422          False            False         False          False   \n",
       "7423          False             True         False          False   \n",
       "7424          False            False         False          False   \n",
       "7425          False            False         False           True   \n",
       "\n",
       "      emotion_joy  emotion_sadness  emotion_shame  \n",
       "0            True            False          False  \n",
       "1           False            False          False  \n",
       "2           False            False          False  \n",
       "3           False             True          False  \n",
       "4           False            False          False  \n",
       "...           ...              ...            ...  \n",
       "7421        False            False          False  \n",
       "7422        False             True          False  \n",
       "7423        False            False          False  \n",
       "7424        False            False           True  \n",
       "7425        False            False          False  \n",
       "\n",
       "[7426 rows x 7 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One-Hot Encoding for all emotions, \n",
    "#instead of just give them numbers, by doing so we will have seven outputs\n",
    "\n",
    "Y = pd.get_dummies(df['EMOTION'], prefix='emotion')\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split into train and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,test_size=0.1,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I didn't check the distributions of emotions at the beginning. Now it just use the simpliest way to divide the dataset. <br>\n",
    "If we get more time we should take a bit more time to analyize the datasets to choose a better way to divide. <br>\n",
    "e.g. there are 40% of texts are all joy while only 5% of them are guilty <br>\n",
    "Then the simple randomized distribution will make a horrible model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything is ready to go and we can design the layers for the model <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMModel:\n",
    "    def __init__(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Bidirectional(LSTM(200, input_shape=(200, 50))))\n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(Dense(7, activation='softmax'))\n",
    "        self.model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    " \n",
    "    def fit(self, X, Y, epochs, batch_size):\n",
    "        self.model.fit(X, Y, epochs=epochs, batch_size=batch_size)\n",
    " \n",
    "    def evaluate(self, X, Y, batch_size):\n",
    "        return self.model.evaluate(X, Y, batch_size=batch_size)\n",
    " \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 95s 3s/step - loss: 1.8717 - accuracy: 0.2478\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# create an instance of the BiLSTMModel class\n",
    "model = BiLSTMModel()\n",
    "\n",
    "# fit the model on the input and target data\n",
    "model.fit(X_train,Y_train, epochs=1, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just addedd some code to save the model \n",
    "\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "CHANGE THE NAME YOU SAVE YOUR MODEL TO\n",
    "\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment this piece of code if you want to save the model \n",
    "\n",
    "#model.model.save('trained_model_Max_20231016_1.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "#del model  # deletes the existing model\n",
    "    \n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "#model = load_model('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_3 (Bidirecti  (None, 400)               401600    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 400)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7)                 2807      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 404407 (1.54 MB)\n",
      "Trainable params: 404407 (1.54 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Take a look of the model layers summarry\n",
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 - 1s - loss: 1.0969 - accuracy: 0.3333 - 1s/epoch - 1s/step\n",
      "Epoch 2/10\n",
      "1/1 - 0s - loss: 1.0960 - accuracy: 0.3333 - 6ms/epoch - 6ms/step\n",
      "Epoch 3/10\n",
      "1/1 - 0s - loss: 1.0951 - accuracy: 0.3333 - 5ms/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "1/1 - 0s - loss: 1.0940 - accuracy: 0.6667 - 7ms/epoch - 7ms/step\n",
      "Epoch 5/10\n",
      "1/1 - 0s - loss: 1.0929 - accuracy: 1.0000 - 5ms/epoch - 5ms/step\n",
      "Epoch 6/10\n",
      "1/1 - 0s - loss: 1.0918 - accuracy: 1.0000 - 5ms/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "1/1 - 0s - loss: 1.0905 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 8/10\n",
      "1/1 - 0s - loss: 1.0891 - accuracy: 1.0000 - 5ms/epoch - 5ms/step\n",
      "Epoch 9/10\n",
      "1/1 - 0s - loss: 1.0875 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 10/10\n",
      "1/1 - 0s - loss: 1.0858 - accuracy: 1.0000 - 5ms/epoch - 5ms/step\n",
      "1/1 [==============================] - 0s 239ms/step\n",
      "Predicted label: 0\n"
     ]
    }
   ],
   "source": [
    "#This was just some example code on how to in theory make such a model, It won't be that relevant anymore\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample training data\n",
    "texts = [\"This is a positive text.\", \"Negative sentiment here.\", \"I feel neutral about this.\"]\n",
    "labels = [1, 0, 2]  # 1 for positive, 0 for negative, 2 for neutral\n",
    "\n",
    "# Tokenize and pad the text data\n",
    "tokenizer = Tokenizer(num_words=1000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "sequences = pad_sequences(sequences, maxlen=10, padding='post', truncating='post')\n",
    "input_data_list = sequences.tolist()\n",
    "\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=1000, output_dim=16, input_length=10))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(3, activation='softmax'))  # Assuming you have 3 classes (positive, negative, neutral)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(input_data_list, labels, epochs=10, verbose=2)\n",
    "\n",
    "# Now, you can use the trained model to predict labels for new input text\n",
    "new_text = [\"I like this product.\"]\n",
    "new_sequence = tokenizer.texts_to_sequences(new_text)\n",
    "new_sequence = pad_sequences(new_sequence, maxlen=10, padding='post', truncating='post')\n",
    "predictions = model.predict(new_sequence)\n",
    "\n",
    "# Get the predicted label (argmax of the softmax output)\n",
    "predicted_label = predictions.argmax()\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
